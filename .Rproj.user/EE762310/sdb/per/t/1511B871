{
    "contents" : "---\ntitle: \"N-Gram Language Model\"\nauthor: \"Fan Li\"\ndate: \"Friday, November 28, 2014\"\noutput: html_document\n---\n\n## Objective\n\nImplement a basic n-gram model using a small subset of us news data.\n\n\n## Key learning\n\n1. Basic n-gram model with Laplace smooth is easy to implement\n2. Multi-core can speed the computation \n3. Still need to further divide the data due to memory limit\n\n\n## Load packages and data\n```{r setup}\nlibrary(\"slam\")  # Sparse matrix\nlibrary(tm)\nlibrary(RWeka)\nlibrary(microbenchmark)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(RColorBrewer)\nlibrary(\"stringr\")\nlibrary(\"doParallel\")\nlibrary(\"foreach\")\nlibrary(\"dplyr\")\n\nf <- file(\"../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt\", \"rb\")\nen_us.news <- readLines(f, encoding=\"UTF-8\")\nclose(f)\n\n# Subset the data for testing\nnews <- en_us.news[sample(length(en_us.news), 10000)]\n#news <- en_us.news[1:10000]  # Use the first 1000000 data for train/test\n```\n\n\n\n## Construct corpus\n\nUse *foreach* package to divide corpus/dtm construction to multiple clusters; then use *do.call* to combine them together.\n\n```{r data_preprocessing}\n# Helper function to preprocess corpus\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\nCorpusPreprocess <- function(corpus){\n  processed.corpus <- corpus %>%\n    tm_map(toSpace, \"/|@|\\\\|()\\\"\") %>%\n    #tm_map(content_transformer(tolower)) %>%\n    tm_map(removeNumbers) %>%\n    #tm_map(removePunctuation) %>%\n    tm_map(stripWhitespace)\n  return(processed.corpus)\n}\n\n# Data preprocessing in parallel\ncore <- 8 # for 4 cores 8 threds\ncl <- makeCluster(core)\nregisterDoParallel(cl)\n\n\nnews.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))\n\nresult.list <- foreach(k = 1:core, .packages=c(\"dplyr\", \"tm\", \"RWeka\")) %dopar% {\n        # Set up Weka Tokenizers\n        BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n        TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n        \n        # Prepare corpus\n        news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()\n        \n        # Tokenization\n        news.dtm.1g <- DocumentTermMatrix(news.corpus, control=list(wordLengths=c(1,Inf)))\n        news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))\n        news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))\n        \n        return(list(news.corpus, news.dtm.1g, news.dtm.2g, news.dtm.3g))\n      }\n  \n\nstopCluster(cl)\n\n# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)\nnews.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))\nnews.dtm.1g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))\nnews.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))\nnews.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))\n\nsave(news.corpus, news.dtm.1g, news.dtm.2g, news.dtm.3g, file=\"../Output/corpus.RData\")\n  \n```\n\n\n\n## Word count\n\n\n```{r frequency_calculation}\n# Helper function to find the most frequent n words\nMostFreq.1<- function(dtm, n=NULL){\n  # dtm: document term matrix\n  # n: if provided, will return the n most frequent words\n  #freq <- colSums(Matrix(dtm, nrow=nrow(dtm), ncol=ncol(dtm), sparse=TRUE))  # Use the Matrix package for more efficient storage, however \n  freq <- colSums(as.matrix(dtm))\n  result <- freq[order(freq, decreasing=TRUE)]\n  if(!is.null(n)){\n    result <- result[1:n]\n  }\n  return(result)\n}\n\nMostFreq<- function(dtm, n=NULL){\n  # dtm: document term matrix\n  # n: if provided, will return the n most frequent words\n  # Use rollup calcuate the colSums: http://stackoverflow.com/a/21921618\n  freq <- rollup(dtm, 1, FUN=sum)  # Note the result is still a simple_triplet_matrix\n  \n  # Convert term frequency list into a \n  result <- setNames(freq$v, freq$dimnames$Terms) \n  result <- result[order(result, decreasing=TRUE)]\n  if(!is.null(n)){\n    result <- result[1:n]\n  }\n  return(result)\n}\n\n\n```\n\nI was going to use Matrix package for more efficient story for sparse matrix. But tm already use the simple_triplet_matrix from slam package underneath.\n\n\n\n## Build ngram model\n\n```{r ngram_laplace_smoothing}\ndict.1gram <- news.dtm.1g %>%\n  #removeSparseTerms(0.999) %>%  \n  MostFreq()  # 189010 terms\n\ndict.2gram <- news.dtm.2g %>%\n  #removeSparseTerms(0.999) %>%  \n  MostFreq()  # 1153335 2gram\n\ndict.3gram <- news.dtm.3g %>%\n  #removeSparseTerms(0.99975) %>% \n  MostFreq()  # 2405615 3gram\n\n\nWordChooser <- function(words, phrase, dict.grams=list(dict.1gram, dict.2gram, dict.3gram)){\n  word.prob <- data_frame(words=tolower(words), gram.1=NA, gram.2=NA, gram.3=NA)\n  \n  phrase <- VCorpus(VectorSource(phrase)) %>% CorpusPreprocess()\n  phrase <- str_split(as.character(phrase[[1]]), \" \")[[1]]\n  \n  if(length(phrase) > 2)\n    phrase <- phrase[(length(phrase)-1): length(phrase)]\n\n  print(phrase)\n  for(i in 1:length(words)){\n    word <- word.prob$words[i]\n    #print(word)\n    word.prob$gram.1[i] <- sum(dict.grams[[1]][word], 1, na.rm=TRUE)/sum(dict.grams[[1]])\n    for(j in 1:length(phrase)){\n      phrase.seg <- paste(phrase[(length(phrase)-j+1): length(phrase)], collapse=\" \")\n      if(!phrase.seg %in% names(dict.grams[[j]])){\n        #print(paste(phrase.seg, \"not in ngrams.\"))\n        next\n      }      \n      #print(dict.grams[[j+1]][paste(phrase.seg, word)])\n      word.prob[i, j+2] <- sum(dict.grams[[j+1]][paste(phrase.seg, word)], 1, na.rm=TRUE)/dict.grams[[j]][phrase.seg]\n      \n    }\n  }\n  return(word.prob)\n}\n\n\nphrase <- \"The guy in front of me just bought a pound of bacon, a bouquet, and a case of\"\nwords <- c(\"cheese\", \"beer\", \"pretzels\", \"soda\")\n\nWordChooser(words, phrase) %>%\n  arrange(-gram.3, -gram.2, -gram.1) \n\nphrase <- \"You're the reason why I smile everyday. Can you follow me please? It would mean the\"\nwords <- c(\"most\", \"best\", \"universe\", \"world\")\n\nWordChooser(words, phrase) %>%\n  arrange(-gram.3, -gram.2, -gram.1) \n\nphrase <- \"Hey sunshine, can you follow me and make me the\"\nwords <- c(\"smelliest\", \"bluest\", \"happiest\", \"saddest\")\n\nWordChooser(words, phrase) %>%\n  arrange(-gram.3, -gram.2, -gram.1) \n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1418178789630.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "767371109",
    "id" : "1511B871",
    "lastKnownWriteTime" : 1418351050,
    "path" : "~/GitHub/DSCapstoneProject/RMD/ngram_model.Rmd",
    "project_path" : "RMD/ngram_model.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}