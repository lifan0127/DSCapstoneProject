{
    "contents" : "---\ntitle: \"corpus_building\"\nauthor: \"Fan Li\"\ndate: \"Thursday, December 11, 2014\"\noutput: html_document\n---\n\n## Progress\n\nCreate n-gram frequency lists using divide-and-conquer (map-reduce) strategy.\n\n\n## Key learning\n\n1. \n2. \n3. \n\n\n## Load packages and data\n```{r setup}\nlibrary(\"slam\")  # Sparse matrix\nlibrary(microbenchmark)\nlibrary(tm)\nlibrary(RWeka)\nlibrary(microbenchmark)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(RColorBrewer)\nlibrary(\"stringr\")\nlibrary(\"doParallel\")\nlibrary(\"foreach\")\nlibrary(\"dplyr\")\n\nf <- file(\"../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt\", \"rb\")\nen_us.news <- readLines(f, encoding=\"UTF-8\")\nclose(f)\n\n# Subset the data for testing\nnews <- en_us.news[sample(length(en_us.news), 1000000)]\n#news <- en_us.news[1:10000]  # Use the first 1000000 data for train/test\n```\n\n```{r helper_functions}\n# Helper function to preprocess corpus\nCorpusPreprocess <- function(corpus){\n  toSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\n  processed.corpus <- corpus %>%\n    tm_map(toSpace, \"/|@|\\\\|()\\\"\") %>%\n    #tm_map(content_transformer(tolower)) %>%\n    tm_map(removeNumbers) %>%\n    #tm_map(removePunctuation) %>%\n    tm_map(stripWhitespace)\n  return(processed.corpus)\n}\n\n\n# Helper function to find the most frequent n words\nMostFreq<- function(dtm, n=NULL){\n  # dtm: document term matrix\n  # n: if provided, will return the n most frequent words\n  # Use rollup calcuate the colSums: http://stackoverflow.com/a/21921618\n  freq <- rollup(dtm, 1, FUN=sum)  # Note the result is still a simple_triplet_matrix\n  \n  # Convert term frequency list into a \n  result <- setNames(freq$v, freq$dimnames$Terms) \n  result <- result[order(result, decreasing=TRUE)]\n  if(!is.null(n)){\n    result <- result[1:n]\n  }\n  return(result)\n}\n\n\n# Helper function to merge frequency lists\nMergeDict <- function(dict.1, dict.2){\n  shared <- names(dict.2) %in% names(dict.1)\n  merged <- dict.1\n  merged[names(dict.2)[shared]] <- merged[names(dict.2)[shared]] + dict.2[names(dict.2)[shared]]\n  merged <- c(merged, dict.2[names(dict.2)[!shared]])\n  return(merged)\n}\n\n```\n\n```{r data_processing}\n# Divide data for batch-processing\nbatch <- 100\nnews.list <- split(news, sample(rep(1:batch, ceiling(length(news)/batch))[1:length(news)]))\n\nfor(i in 1:10){\n  # Data preprocessing in parallel\n  core <- 8 # for 4 cores 8 threds\n  cl <- makeCluster(core)\n  registerDoParallel(cl)\n  \n  news.item <- split(news.list[[i]], sample(rep(1:core, ceiling(length(news.list[[i]])/core))[1:length(news.list[[i]])]))\n  \n  result.list <- foreach(k = 1:core, .packages=c(\"dplyr\", \"tm\", \"RWeka\")) %dopar% {\n        # Set up Weka Tokenizers\n        BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n        TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n        \n        # Prepare corpus\n        news.corpus <- VCorpus(VectorSource(news.item[[k]])) %>% CorpusPreprocess()\n        \n        # Tokenization\n        news.dtm.1g <- DocumentTermMatrix(news.corpus, control=list(wordLengths=c(1,Inf)))\n        news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))\n        news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))\n        \n        return(list(news.dtm.1g, news.dtm.2g, news.dtm.3g))\n      }\n  stopCluster(cl)\n   \n  assign(paste0(\"dict.1gram.\", i), do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]])) %>% MostFreq()) \n  assign(paste0(\"dict.2gram.\", i), do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]])) %>% MostFreq())\n  assign(paste0(\"dict.3gram.\", i), do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]])) %>% MostFreq())\n  #browser()\n  do.call(save, list(paste0(\"dict.1gram.\", i), file=paste0(\"../Output/Batch/1gram\", i, \".RData\")))\n  do.call(save, list(paste0(\"dict.2gram.\", i), file=paste0(\"../Output/Batch/2gram\", i, \".RData\")))\n  do.call(save, list(paste0(\"dict.3gram.\", i), file=paste0(\"../Output/Batch/3gram\", i, \".RData\")))\n  rm(list=c(paste0(\"dict.1gram.\", i),\n            paste0(\"dict.2gram.\", i),\n            paste0(\"dict.3gram.\", i)))\n}\n\n```\n\n```{r data_merge}\nload(\"../Output/Batch/1gram1.RData\")\nload(\"../Output/Batch/2gram1.RData\")\nload(\"../Output/Batch/3gram1.RData\")\nmerged.1gram <- dict.1gram.1\nmerged.2gram <- dict.2gram.1\nmerged.3gram <- dict.3gram.1\nrm(list=c(\"dict.1gram.1\", \"dict.2gram.1\", \"dict.3gram.1\"))\nunique.terms <- data.frame(batch=1, \n                           gram1=length(merged.1gram), \n                           gram2=length(merged.2gram), \n                           gram3=length(merged.3gram), \n                           time=0)\n\nfor(i in 2:batch){\n  time <- microbenchmark({\n      load(paste0(\"../Output/Batch/1gram\", i, \".RData\"))\n      load(paste0(\"../Output/Batch/2gram\", i, \".RData\"))\n      load(paste0(\"../Output/Batch/3gram\", i, \".RData\"))\n      merged.1gram <- MergeDict(merged.1gram, get(paste0(\"dict.1gram.\", i)))\n      merged.2gram <- MergeDict(merged.2gram, get(paste0(\"dict.2gram.\", i)))\n      merged.3gram <- MergeDict(merged.3gram, get(paste0(\"dict.3gram.\", i)))\n      rm(list=c(paste0(\"dict.1gram.\", i), paste0(\"dict.2gram.\", i), paste0(\"dict.3gram.\", i)))},\n    times=1L, unit=\"s\")\n\n  unique.terms <- rbind(unique.terms,\n                        data.frame(batch=i, \n                                   gram1=length(merged.1gram), \n                                   gram2=length(merged.2gram), \n                                   gram3=length(merged.3gram),\n                                   time=time$time))\n}\n\nggplot(melt(unique.terms[, 1:4], id=\"batch\", variable.name=\"ngram.type\", value.name=\"count\"), aes(x=batch, y=count, group=ngram.type, color=ngram.type)) +\n  geom_line(size=1.5) +\n  geom_point(size=4)\n\nggplot(unique.terms[, c(1,5)], aes(x=batch, y=time)) + \n  geom_point(size=4) +\n  geom_smooth(method=\"loess\", size=1.5)\n  \n\nsave(merged.1gram, file=\"../Output/1gram.RData\")\nsave(merged.2gram, file=\"../Output/2gram.RData\")\nsave(merged.3gram, file=\"../Output/3gram.RData\")\n\n```\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1418350119259.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1796702035",
    "id" : "40510DF2",
    "lastKnownWriteTime" : 1418442984,
    "path" : "~/GitHub/DSCapstoneProject/RMD/corpus_building.Rmd",
    "project_path" : "RMD/corpus_building.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 3,
    "source_on_save" : false,
    "type" : "r_markdown"
}