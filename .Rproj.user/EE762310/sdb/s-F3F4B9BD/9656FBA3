{
    "contents" : "# https://class.coursera.org/dsscapstone-002/wiki/Task_2\n\nlibrary(tm)\nlibrary(RWeka)\nlibrary(dplyr)\n\nproduction.mode = FALSE\n\nif(production.mode){\n  warning(\"Will reload the full dataset\")\n}else{\n  load(file=\"../../DSCapstoneProject_Data/Output/RData/task_1_blogs_news_twitter_sample.RData\")\n}\n\n\n# Helper function to preprocess corpus\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\ncorpus.preprocess <- function(corpus){\n  processed.corpus <- corpus %>%\n    tm_map(toSpace, \"/|@|\\\\|\") %>%\n    tm_map(content_transformer(tolower)) %>%\n    tm_map(removeNumbers) %>%\n    tm_map(removePunctuation) %>%\n    tm_map(removeWords, stopwords(\"english\")) %>%\n    tm_map(stripWhitespace)\n  return(processed.corpus)\n}\n\n\n# Create corpus and apply preprocessing\nblogs.corpus <- VCorpus(VectorSource(blogs)) %>% corpus.preprocess()\nnews.corpus <- VCorpus(VectorSource(news)) %>% corpus.preprocess()\ntwitter.corpus <- VCorpus(VectorSource(twitter)) %>% corpus.preprocess()\n\n\n# Create document term matrix (dtm)\nblogs.dtm <- DocumentTermMatrix(blogs.corpus) %>% removeSparseTerms(0.95)\nnews.dtm <- DocumentTermMatrix(news.corpus) %>% removeSparseTerms(0.95)\ntwitter.dtm <- DocumentTermMatrix(twitter.corpus) %>% removeSparseTerms(0.99)\n\n\n# Frequnt words\n# Helper function to find the most frequent n words\nmost.freq <- function(dtm, n=5){\n  freq <- colSums(as.matrix(dtm))\n  return(freq[order(freq, decreasing=TRUE)][1:n])\n}\n\n# Blogs frequency: one will  can like just \nmost.freq(blogs.dtm)\n\n# News frequency: said will  one  new  can\nmost.freq(news.dtm)\n\n# Twitter frequency: just like love  get good\nmost.freq(twitter.dtm)\n\n\n\n\n# Create 2-gram\nBigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n\nblogs.dtm.2g <- DocumentTermMatrix(blogs.corpus, control=list(tokenize = BigramTokenizer)) %>% removeSparseTerms(0.995) \nnews.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)) %>% removeSparseTerms(0.995) \ntwitter.dtm.2g <- DocumentTermMatrix(twitter.corpus, control=list(tokenize = BigramTokenizer)) %>% removeSparseTerms(0.999) \n\n# Blogs frequency 2gram: last year   new york  dont know  years ago first time \nmost.freq(blogs.dtm.2g)\n\n# News frequency 2gram: last year    new york high school    st louis   years ago\nmost.freq(news.dtm.2g)\n\n# Twitter frequency 2gram: right now       cant wait       dont know      last night looking forward\nmost.freq(twitter.dtm.2g)\n\n\n\n\n# Create 3-gram\nTrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n\nblogs.dtm.3g <- DocumentTermMatrix(blogs.corpus, control=list(tokenize = TrigramTokenizer)) %>% removeSparseTerms(0.9995) \nnews.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)) %>% removeSparseTerms(0.9995) \ntwitter.dtm.3g <- DocumentTermMatrix(twitter.corpus, control=list(tokenize = TrigramTokenizer)) %>% removeSparseTerms(0.9999) \n\n# Blogs frequency 3gram: \nmost.freq(blogs.dtm.3g)\n\n# News frequency 3gram: \nmost.freq(news.dtm.3g)\n\n# Twitter frequency 3gram: \nmost.freq(twitter.dtm.3g)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1416173135610.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1619617523",
    "id" : "9656FBA3",
    "lastKnownWriteTime" : 1416195810,
    "path" : "~/GitHub/DSCapstoneProject/R/task_2_exploratory_analysis.R",
    "project_path" : "R/task_2_exploratory_analysis.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}