{
    "contents" : "---\ntitle: \"tm Performance Optimization\"\nauthor: \"Fan Li\"\ndate: \"Thursday, November 27, 2014\"\noutput: html_document\n---\n\nThe tm package is the main work horse for this project. However its performance, especially on large dataset, is problematic and significantl inferior to NLTK in Python.^[\\tiny http://bommaritollc.com/2011/02/pre-processing-text-rtm-vs-pythonnltk/]\n\n\n## Performance Evaluation\n\n### Build corpus\n\n```{r setup}\nlibrary(tm)\nlibrary(RWeka)\nlibrary(microbenchmark)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(RColorBrewer)\nlibrary(dplyr)\n\nf <- file(\"../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt\", \"rb\")\nen_us.news <- readLines(f)\nclose(f)\n\n```\n\n\nThe *en_us.news* dataset contains `r length(en_us.news)` lines. Below are the test results showing time needed to convert and preprocess corpus from data of 10,000 to 1,000,000 lines.\n\n```{r speed_test}\n# Helper function to preprocess corpus\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\ncorpus.preprocess <- function(corpus){\n  processed.corpus <- corpus %>%\n    tm_map(toSpace, \"/|@|\\\\|\") %>%\n    tm_map(content_transformer(tolower)) %>%\n    tm_map(removeNumbers) %>%\n    tm_map(removePunctuation) %>%\n    tm_map(removeWords, stopwords(\"english\")) %>%\n    tm_map(stripWhitespace)\n  return(processed.corpus)\n}\n\n\ntime.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)\n\nfor(i in seq(100000, 1000000, 100000)){\n  # Time for corpus construction\n  corpus <- microbenchmark(\n    news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),\n    times=1L, unit=\"s\")\n  \n  # Time for dtm constructurtion\n  dtm <- microbenchmark(\n    news.dtm <- DocumentTermMatrix(news.corpus),\n    times=1L, unit=\"s\")\n  \n  # Time for 2gram dtm\n  BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n  dtm.2g <- microbenchmark(\n    news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),\n    times=1L, unit=\"s\")\n  \n  # Time for 3gram dtm\n  TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n  dtm.3g <- microbenchmark(\n    news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),\n    times=1L, unit=\"s\")\n\n\n  time.record <- rbind(time.record,\n                         data.frame(data.size = i, \n                                    corpus.time = corpus$time, \n                                    dtm.time = dtm$time,\n                                    dtm.2g.time = dtm.2g$time,\n                                    dtm.3g.time = dtm.3g$time))\n}\n\nggplot(melt(time.record, id=\"data.size\", value.name=\"time.needed\"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +\n  geom_line(size=1) +\n  geom_point(size=4) +\n  theme_bw() + \n  ggtitle(\"Impact of data size on computation time\")\n\n```\n\n\nParalellization using *foreach* package was tested for the corpus and dtm construction. Below are the results shwoing the impacts of CPU cores (threads) and data size (1,000 - 10,000). \n\n```{r speed_test_foreach}\nlibrary(doParallel)\nlibrary(foreach)\n\ntime.parallel <- data.frame(core=integer(), data.size=integer(), time=numeric())\n\nfor(i in 1:8){\n  cl <- makeCluster(i)\n  registerDoParallel(cl)\n  for(j in seq(1000, 10000, 1000)){\n    \n      print(c(i, j))\n      test.result.parallel <- microbenchmark({\n        news.list <- split(en_us.news[1:j], sample(rep(1:i, ceiling(j/i))[1:j]))\n        \n        result.list <- foreach(k = 1:i, .packages=c(\"dplyr\", \"tm\", \"RWeka\")) %dopar% {\n  \n          BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n          TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n          news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% corpus.preprocess()\n          news.dtm <- DocumentTermMatrix(news.corpus)\n          news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))\n          news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))\n          list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)\n        }\n        \n        # Merge resulting dtm's\n        news.corpus.combined <- NULL\n        news.dtm.2g.combined <- NULL\n        news.dtm.3g.combined <- NULL\n        combined <- lapply(result.list, function(x){\n          news.corpus.combined <- c(news.corpus.combined, x[[1]])\n          news.dtm.2g.combined <- c(news.dtm.2g.combined, x[[2]])\n          news.dtm.3g.combined <- c(news.dtm.3g.combined, x[[3]])\n        })\n      }, times=1L)\n      time.parallel <- rbind(time.parallel, \n                             data.frame(core=i, data.size=j, time=test.result.parallel$time))\n  }\n  stopCluster(cl)\n}\n\nggplot(time.parallel, aes(x=data.size, y=time, group=factor(core), color=factor(core))) +\n  geom_line(size=1) +\n  geom_point(size=4) +\n  scale_color_brewer() +\n  theme_bw() + \n  ggtitle(\"Impact of CPU (core/thread) and data size on parallelization\")\n\n\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1417198738985.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1293140453",
    "id" : "60ADE1F2",
    "lastKnownWriteTime" : 1417197609,
    "path" : "~/GitHub/DSCapstoneProject/RMD/tm_optimization.Rmd",
    "project_path" : "RMD/tm_optimization.Rmd",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "type" : "r_markdown"
}