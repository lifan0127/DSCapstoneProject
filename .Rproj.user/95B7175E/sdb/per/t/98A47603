{
    "contents" : "---\ntitle: \"N-Gram Language Model\"\nauthor: \"Fan Li\"\ndate: \"Friday, November 28, 2014\"\noutput: html_document\n---\n\nIn this decoment, we will implement a basic N-gram model based on:\n\n1. \n2. NLTK\n\n```{r setup}\nlibrary(tm)\nlibrary(RWeka)\nlibrary(microbenchmark)\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(RColorBrewer)\nlibrary(\"doParallel\")\nlibrary(\"foreach\")\nlibrary(\"dplyr\")\n\nf <- file(\"../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt\", \"rb\")\nen_us.news <- readLines(f, encoding=\"UTF-8\")\nclose(f)\n\nnews <- en_us.news[sample(length(en_us.news), 10000)]\n\n```\n\nUse *foreach* package to divide corpus/dtm construction to multiple clusters; then use *do.call* to combine them together.\n\n```{r data_preprocessing}\n# Helper function to preprocess corpus\ntoSpace <- content_transformer(function(x, pattern) gsub(pattern, \" \", x))\nCorpusPreprocess <- function(corpus){\n  processed.corpus <- corpus %>%\n    tm_map(toSpace, \"/|@|\\\\|\") %>%\n    tm_map(content_transformer(tolower)) %>%\n    tm_map(removeNumbers) %>%\n    tm_map(removePunctuation) %>%\n    tm_map(stripWhitespace)\n  return(processed.corpus)\n}\n\n# Data preprocessing in parallel\ncore <- 4\ncl <- makeCluster(core)\nregisterDoParallel(cl)\n  \nnews.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))\n\nresult.list <- foreach(k = 1:core, .packages=c(\"dplyr\", \"tm\", \"RWeka\")) %dopar% {\n        BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))\n        TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))\n        news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()\n        news.dtm <- DocumentTermMatrix(news.corpus, control=list(wordLengths=c(1,Inf)))\n        news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))\n        news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))\n        list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)\n      }\nstopCluster(cl)\n\n# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)\nnews.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))\nnews.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))\nnews.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))\nnews.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))\n  \n```\n\n\n\n```{r frequency_calculation}\n# Helper function to find the most frequent n words\nMostFreq <- function(dtm, n=NULL){\n  freq <- colSums(as.matrix(dtm))\n  result <- freq[order(freq, decreasing=TRUE)]\n  if(!is.null(n)){\n    result <- result[1:n]\n  }\n  return(result)\n}\n\n```\n\n\n```{r ngram_laplace_smoothing}\ndict.1gram <- news.dtm %>%\n  removeSparseTerms(0.999) %>%  # 3321 1-grams\n  MostFreq()\n\ndict.2gram <- news.dtm.2g %>%\n  removeSparseTerms(0.9995) %>%  # 7035 2-grams\n  MostFreq()\n\ndict.3gram <- news.dtm.3g %>%\n  removeSparseTerms(0.99975) %>%  # 4697 2-grams\n  MostFreq()\n\n\n\n\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
    "created" : 1417198796199.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "298992223",
    "id" : "98A47603",
    "lastKnownWriteTime" : 1417214451,
    "path" : "~/GitHub/DSCapstoneProject/RMD/ngram_model.Rmd",
    "project_path" : "RMD/ngram_model.Rmd",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 2,
    "source_on_save" : false,
    "type" : "r_markdown"
}