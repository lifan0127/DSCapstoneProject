unlist(a)
a
a <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
a
?do.call
a <- do.call(function(x) c(x, recursive = TRUE), lapply(result.list, function(x) x[[1]]))
a <- do.call(function(y) c(y, recursive = TRUE), lapply(result.list, function(x) x[[1]]))
a <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
news.dtm
# Helper function to preprocess corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
corpus.preprocess <- function(corpus){
processed.corpus <- corpus %>%
tm_map(toSpace, "/|@|\\|") %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(removeWords, stopwords("english")) %>%
tm_map(stripWhitespace)
return(processed.corpus)
}
# Data preprocessing in parallel
core <- 4
cl <- makeCluster(core)
registerDoParallel(cl)
news.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))
result.list <- foreach(k = 1:core, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% corpus.preprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
stopCluster(cl)
# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
news.dtm.2g[[1]]
class(news.dtm.3g)
# Helper function to find the most frequent n words
most.freq <- function(dtm, n=10){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)][1:n]
return(data_frame(term=names(result), count=result))
}
ggplot(most.freq(news.dtm), aes(x=reorder(term, -count), y=count)) +
geom_bar(stat="identity") +
theme_bw() +
theme(axis.title.x = element_blank()) +
ggtitle("Most frequent words in news")
ggplot(most.freq(news.dtm %>% removeSparseTerms(0.9999)), aes(x=reorder(term, -count), y=count)) +
geom_bar(stat="identity") +
theme_bw() +
theme(axis.title.x = element_blank()) +
ggtitle("Most frequent words in news")
# Helper function to find the most frequent n words
MostFreq <- function(dtm, n=10){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)][1:n]
return(data_frame(term=names(result), count=result))
}
news.dtm %>%
removeSparseTerms(0.9999) %>%
MostFreq() %>%
ggplot(aes(x=reorder(term, -count), y=count)) +
geom_bar(stat="identity") +
theme_bw() +
theme(axis.title.x = element_blank()) +
ggtitle("Most frequent words in news")
# Helper function to find the most frequent n words
MostFreq <- function(dtm, n=10){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)][1:n]
return(data_frame(term=names(result), count=result))
}
news.dtm %>%
removeSparseTerms(0.99999) %>%
MostFreq() %>%
ggplot(aes(x=reorder(term, -count), y=count)) +
geom_bar(stat="identity") +
theme_bw() +
theme(axis.title.x = element_blank()) +
ggtitle("Most frequent words in news")
news.dtm %>%
removeSparseTerms(0.99999)
news.dtm
news.dtm %>%
removeSparseTerms(0.99)
# Helper function to find the most frequent n words
MostFreq <- function(dtm, n=10){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)][1:n]
return(data_frame(term=names(result), count=result))
}
news.dtm %>%
removeSparseTerms(0.99) %>%
MostFreq() %>%
ggplot(aes(x=reorder(term, -count), y=count)) +
geom_bar(stat="identity") +
theme_bw() +
theme(axis.title.x = element_blank()) +
ggtitle("Most frequent words in news")
news.dtm %>%
removeSparseTerms(0.99) %>%
MostFreq()
news.dtm.2g[[1]]
news.dtm.2g
MostFreq <- function(dtm, n=NULL){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)]
if(!is.null(n)){
result <- result[1:n]
}
return(data_frame(term=names(result), count=result))
}
dict.1gram <- MostFreq(news.dtm)
dict.1gram <- news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
dim(dict.1gram)
head(dict.1gram)
?setNames
# Helper function to find the most frequent n words
MostFreq <- function(dtm, n=NULL){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)]
if(!is.null(n)){
result <- result[1:n]
}
return(result)
}
dict.1gram <- news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
dict.1gram[1:5]
dict.1gram$absdaf
dict.1gram["adfgdas"]
dict.1gram["an"]
dict.1gram[["an"]]
head(dict.1gram)
dict.1gram["said"]
# Helper function to preprocess corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
CorpusPreprocess <- function(corpus){
processed.corpus <- corpus %>%
tm_map(toSpace, "/|@|\\|") %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
return(processed.corpus)
}
# Data preprocessing in parallel
core <- 4
cl <- makeCluster(core)
registerDoParallel(cl)
news.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))
result.list <- foreach(k = 1:core, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
stopCluster(cl)
# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
dict.1gram <- news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
dict.1gram["an"]
dict.1gram[1:10]
dict.1gram["the"]
dict.1gram["a"]
names(news.dtm)
colnames(news.dtm)
"an" %in% colnames(news.dtm)
"a" %in% colnames(news.dtm)
news
news.corpus[1]
news.corpus[[1]]
news.corpus[[2]]
news.corpus
news.corpus[[10000]]
news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
length(news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq())
dict.1gram <- news.dtm %>%
removeSparseTerms(0.9999) %>%
MostFreq()
news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
```{r ngram_laplace_smoothing}
dict.1gram <- news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
dict.1gram <- news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
length(dict.1gram)
as.matrix(dtm)
a <- as.matrix(news.dtm)
news <- en_us.news[sample(length(en_us.news), 1000)]
# Helper function to preprocess corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
CorpusPreprocess <- function(corpus){
processed.corpus <- corpus %>%
tm_map(toSpace, "/|@|\\|") %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
return(processed.corpus)
}
# Data preprocessing in parallel
core <- 4
cl <- makeCluster(core)
registerDoParallel(cl)
news.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))
result.list <- foreach(k = 1:core, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
stopCluster(cl)
# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
# Helper function to find the most frequent n words
MostFreq <- function(dtm, n=NULL){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)]
if(!is.null(n)){
result <- result[1:n]
}
return(result)
}
as.matrix(news.dtm)
dict.1gram <- news.dtm %>%
MostFreq()
dict.1gram["a"]
dict.1gram
names(dict.1gram)
dict.1gram["an"]
dict.1gram{"moro"}
dict.1gram["moro"]
news.corpus
news <- en_us.news[sample(length(en_us.news), 100)]
# Data preprocessing in parallel
core <- 4
cl <- makeCluster(core)
registerDoParallel(cl)
news.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))
result.list <- foreach(k = 1:core, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
stopCluster(cl)
# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
dict.1gram <- news.dtm %>%
MostFreq()
as.matrix(news.dtm)
dict.1gram
dict.1gram <- news.dtm %>%
MostFreq()
dict.1gram
dict.1gram["retrieve"]
dict.1gram["the"]
dict.1gram["an"]
news.corpus
news.corpus[1]
news.corpus[[1]]
news.corpus[[2]]
news.dtm[[2]]
news <- en_us.news[sample(length(en_us.news), 10)]
# Helper function to preprocess corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
CorpusPreprocess <- function(corpus){
processed.corpus <- corpus %>%
tm_map(toSpace, "/|@|\\|") %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
return(processed.corpus)
}
# Data preprocessing in parallel
core <- 4
cl <- makeCluster(core)
registerDoParallel(cl)
news.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))
result.list <- foreach(k = 1:core, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
stopCluster(cl)
# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
news.corpus
news.corpus[[10]]
names(news.dtm)
colnames(news.dtm)
"test" %in% colnames(news.dtm)
"a" %in% colnames(news.dtm)
news
a <- VCorpus(VectorSource(news)) %>% CorpusPreprocess()
a
a[[1]]
a[[2]]
a[[3]]
a[[4]]
news.corpus <- VCorpus(VectorSource(news)) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm
colnames(news.dtm)
news.corpus[[2]]
?readLines
f <- file("../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt", "rb")
en_us.news <- readLines(f, encoding="UTF-8")
close(f)
news <- en_us.news[sample(length(en_us.news), 10)]
news
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
CorpusPreprocess <- function(corpus){
processed.corpus <- corpus %>%
tm_map(toSpace, "/|@|\\|") %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
return(processed.corpus)
}
news.corpus <- VCorpus(VectorSource(news)) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
colnames(news.dtm)
CorpusPreprocess
news.corpus[[1]]
news.corpus[[2]]
news.corpus[[3]]
news[[2]]
news.dtm <- DocumentTermMatrix(news.corpus)
colnames(news.dtm)
dim(as.matrix(news.dtm))
news.corpus[[2]]
DocumentTermMatrix(news.corpus[[2]])
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm
as.matrix(news.dtm)
DocumentTermMatrix(news.corpus)
?DocumentTermMatrix
news.dtm <- DocumentTermMatrix(news.corpus, control=list(wordLengths=c(1,Inf)))
colnames(news.dtm)
library(tm)
library(RWeka)
library(microbenchmark)
library(ggplot2)
library(reshape2)
library(RColorBrewer)
library("doParallel")
library("foreach")
library("dplyr")
f <- file("../../DSCapstoneProject_Data/Data/final/en_US/en_US.news.txt", "rb")
en_us.news <- readLines(f, encoding="UTF-8")
close(f)
news <- en_us.news[sample(length(en_us.news), 10000)]
# Helper function to preprocess corpus
toSpace <- content_transformer(function(x, pattern) gsub(pattern, " ", x))
CorpusPreprocess <- function(corpus){
processed.corpus <- corpus %>%
tm_map(toSpace, "/|@|\\|") %>%
tm_map(content_transformer(tolower)) %>%
tm_map(removeNumbers) %>%
tm_map(removePunctuation) %>%
tm_map(stripWhitespace)
return(processed.corpus)
}
# Data preprocessing in parallel
core <- 4
cl <- makeCluster(core)
registerDoParallel(cl)
news.list <- split(news, sample(rep(1:core, ceiling(length(news)/core))[1:length(news)]))
result.list <- foreach(k = 1:core, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% CorpusPreprocess()
news.dtm <- DocumentTermMatrix(news.corpus, control=list(wordLengths=c(1,Inf)))
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
stopCluster(cl)
# Merge corpus and dtm's (http://stackoverflow.com/a/20971354)
news.corpus <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[1]]))
news.dtm <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[2]]))
news.dtm.2g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[3]]))
news.dtm.3g <- do.call(function(...) c(..., recursive = TRUE), lapply(result.list, function(x) x[[4]]))
# Helper function to find the most frequent n words
MostFreq <- function(dtm, n=NULL){
freq <- colSums(as.matrix(dtm))
result <- freq[order(freq, decreasing=TRUE)]
if(!is.null(n)){
result <- result[1:n]
}
return(result)
}
dict.1gram <- news.dtm %>%
MostFreq()
news.dtm %>%
removeSparseTerms(0.9999)
news.dtm
news.dtm %>%
removeSparseTerms(0.999)
dict.1gram <- news.dtm %>%
removeSparseTerms(0.999) %>%
MostFreq()
dict.1gram["an"]
dict.1gram["an"]
dict.1gram["a"]
news.dtm.2g
news.dtm.2g %>%
removeSparseTerms(0.9995)
news.dtm.2g %>%
removeSparseTerms(0.999)
news.dtm.2g %>%
removeSparseTerms(0.9999)
news.dtm.3g
news.dtm.3g %>%
removeSparseTerms(0.9999)
news.dtm.3g
dict.2gram <- news.dtm.2g %>%
removeSparseTerms(0.9999) %>%  # 186008 2-grams
MostFreq()
dict.2gram <- news.dtm.2g %>%
removeSparseTerms(0.9995) %>%  # 186008 2-grams
MostFreq()
news.dtm.2g %>%
removeSparseTerms(0.9995)
news.dtm.3g %>%
removeSparseTerms(0.9999)
news.dtm.3g %>%
removeSparseTerms(0.9995)
news.dtm.3g %>%
removeSparseTerms(0.9998)
dict.3gram <- news.dtm.3g %>%
removeSparseTerms(0.9998) %>%  # 186008 2-grams
MostFreq()
dict.3gram <- news.dtm.3g %>%
removeSparseTerms(0.9997) %>%  # 186008 2-grams
MostFreq()
news.dtm.3g %>%
removeSparseTerms(0.9997)
dict.3gram <- news.dtm.3g %>%
removeSparseTerms(0.99975) %>%  # 186008 2-grams
MostFreq()
news.dtm.3g %>%
removeSparseTerms(0.99975)
news.dtm.3g %>%
removeSparseTerms(0.9998)
env.1gram <- new.env(hash=TRUE)
env.1gram[[names(dict.1gram)]] <- dict.1gram
env.1gram[[names(dict.1gram)]] <- 1
assign(names(dict.1gram), envir=env.1gram) <- dict.1gram
env.1gram <- new.env(hash=TRUE)
assign(names(dict.1gram), envir=env.1gram) <- dict.1gram
assign(names(dict.1gram), dict.1gram, envir=env.1gram)
lapply(dict.1gram, function(x) assign(names(x), x, envir=env.1gram))
apply(dict.1gram, function(x) assign(names(x), x, envir=env.1gram))
env.1gram <- new.env(hash=TRUE)
lapply(dict.1gram, function(x) assign(names(x), x, envir=env.1gram))
lapply(dict.1gram, function(x) print(names(x)))
names(dict.1gram[1])
names(dict.1gram[[1]])
