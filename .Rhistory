data.frame(core=i, data.size=j, time=test.result.parallel[3]))
}
stopCluster(cl)
}
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- system.time({  # Note: microbenchmark creates error
result <- foreach(i = 1:nrow(mtcars)) %dopar% rowsum(mtcars[i, ])
})
}
stopCluster(cl)
?rowSums
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- system.time({  # Note: microbenchmark creates error
result <- foreach(i = 1:nrow(mtcars)) %dopar% rowSums(mtcars[i, ])
})
}
stopCluster(cl)
library(doParallel)
library(foreach)
time.parallel <- data.frame(core=integer(), data.size=integer(), time=numeric())
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- system.time({  # Note: microbenchmark creates error
result <- foreach(i = 1:nrow(mtcars)) %dopar%
rowSums(mtcars[i, ])
})
}
stopCluster(cl)
library(doParallel)
library(foreach)
time.parallel <- data.frame(core=integer(), data.size=integer(), time=numeric())
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- system.time({  # Note: microbenchmark creates error
result <- foreach(i = 1:nrow(mtcars)) %dopar%
rowSums(mtcars[i, ])
})
stopCluster(cl)
test.time
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- microbenchmark(  # Note: microbenchmark creates error
result <- foreach(i = 1:nrow(mtcars)) %dopar%
rowSums(mtcars[i, ])
)
stopCluster(cl)
result
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- microbenchmark(  # Note: microbenchmark creates error
result <- foreach(i = 1:nrow(mtcars)) %dopar% {
rowSums(mtcars[i, ])
}
)
stopCluster(cl)
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- microbenchmark(  # Note: microbenchmark creates error
print("hello")
result <- foreach(i = 1:nrow(mtcars)) %dopar% {
rowSums(mtcars[i, ])
}
)
stopCluster(cl)
# microbenchmark
cl <- makeCluster(1)
registerDoParallel(cl)
test.time <- microbenchmark({  # Note: microbenchmark creates error
print("hello")
result <- foreach(i = 1:nrow(mtcars)) %dopar% {
rowSums(mtcars[i, ])
}
})
library(doParallel)
library(foreach)
time.parallel <- data.frame(core=integer(), data.size=integer(), time=numeric())
for(i in 1:8){
cl <- makeCluster(i)
registerDoParallel(cl)
for(j in seq(100, 500, 100)){
print(c(i, j))
test.result.parallel <- microbenchmark({
news.list <- split(en_us.news[1:j], sample(rep(1:i, ceiling(j/i))[1:j]))
result.list <- foreach(k = 1:i, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% corpus.preprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
# Merge resulting dtm's
news.corpus.combined <- NULL
news.dtm.2g.combined <- NULL
news.dtm.3g.combined <- NULL
combined <- lapply(result.list, function(x){
news.corpus.combined <- c(news.corpus.combined, x[[1]])
news.dtm.2g.combined <- c(news.dtm.2g.combined, x[[2]])
news.dtm.3g.combined <- c(news.dtm.3g.combined, x[[3]])
})
})
time.parallel <- rbind(time.parallel,
data.frame(core=i, data.size=j, time=test.result.parallel$time))
}
stopCluster(cl)
}
library(doParallel)
library(foreach)
time.parallel <- data.frame(core=integer(), data.size=integer(), time=numeric())
for(i in 1:8){
cl <- makeCluster(i)
registerDoParallel(cl)
for(j in seq(100, 500, 100)){
print(c(i, j))
test.result.parallel <- microbenchmark({
news.list <- split(en_us.news[1:j], sample(rep(1:i, ceiling(j/i))[1:j]))
result.list <- foreach(k = 1:i, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% corpus.preprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
# Merge resulting dtm's
news.corpus.combined <- NULL
news.dtm.2g.combined <- NULL
news.dtm.3g.combined <- NULL
combined <- lapply(result.list, function(x){
news.corpus.combined <- c(news.corpus.combined, x[[1]])
news.dtm.2g.combined <- c(news.dtm.2g.combined, x[[2]])
news.dtm.3g.combined <- c(news.dtm.3g.combined, x[[3]])
})
}, times=1L)
time.parallel <- rbind(time.parallel,
data.frame(core=i, data.size=j, time=test.result.parallel$time))
}
stopCluster(cl)
}
head(test.result.parallel)
time.parallel
head(time.parallel)
ggplot(time.parallel, aes(x=data.size, y=time, group=core, color=core)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw()
ggplot(time.parallel, aes(x=data.size, y=time, group=factor(core), color=factor(core))) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw()
library(RColorBrewer)
ggplot(time.parallel, aes(x=data.size, y=time, group=factor(core), color=factor(core))) +
geom_line(size=1) +
geom_point(size=4) +
scale_fill_brewer(palette="Set1") +
theme_bw()
ggplot(time.parallel, aes(x=data.size, y=time, group=factor(core), color=factor(core))) +
geom_line(size=1) +
geom_point(size=4) +
scale_fill_brewer() +
theme_bw()
ggplot(time.parallel, aes(x=data.size, y=time, group=factor(core), color=factor(core))) +
geom_line(size=1) +
geom_point(size=4) +
scale_color_brewer() +
theme_bw()
library(doParallel)
library(foreach)
time.parallel <- data.frame(core=integer(), data.size=integer(), time=numeric())
for(i in 1:8){
cl <- makeCluster(i)
registerDoParallel(cl)
for(j in seq(1000, 10000, 1000)){
print(c(i, j))
test.result.parallel <- microbenchmark({
news.list <- split(en_us.news[1:j], sample(rep(1:i, ceiling(j/i))[1:j]))
result.list <- foreach(k = 1:i, .packages=c("dplyr", "tm", "RWeka")) %dopar% {
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
news.corpus <- VCorpus(VectorSource(news.list[[k]])) %>% corpus.preprocess()
news.dtm <- DocumentTermMatrix(news.corpus)
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer))
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer))
list(news.corpus, news.dtm, news.dtm.2g, news.dtm.3g)
}
# Merge resulting dtm's
news.corpus.combined <- NULL
news.dtm.2g.combined <- NULL
news.dtm.3g.combined <- NULL
combined <- lapply(result.list, function(x){
news.corpus.combined <- c(news.corpus.combined, x[[1]])
news.dtm.2g.combined <- c(news.dtm.2g.combined, x[[2]])
news.dtm.3g.combined <- c(news.dtm.3g.combined, x[[3]])
})
}, times=1L)
time.parallel <- rbind(time.parallel,
data.frame(core=i, data.size=j, time=test.result.parallel$time))
}
stopCluster(cl)
}
ggplot(time.parallel, aes(x=data.size, y=time, group=factor(core), color=factor(core))) +
geom_line(size=1) +
geom_point(size=4) +
scale_color_brewer() +
theme_bw()
?microbenchmark
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L)
#   time.record <- rbind(time.record,
#                          data.frame(data.size = i,
#                                     corpus.time = corpus$time,
#                                     dtm.time = dtm$time,
#                                     dtm.2g.time = dtm.2g$time,
#                                     dtm.3g.time = dtm.3g$time))
}
time
str(time)
time$time
time[1]
time[[1]]
time$time[1]
time$time[1]/1000
time$unit
time$Unit
class(time)
attr
attr(time)
convert_to_unit(time)
library(‘microbenchmark’
)
library(microbenchmark)
install.packages("microbenchmark")
install.packages("microbenchmark")
install.packages("microbenchmark")
library(microbenchmark)
convert_to_unit(time)
convert_to_unit
?convert_to_unit
convert_to_unit(time, unit="s")
time
microbenchmark::convert_to_unit(time, unit="s")
library(microbenchmark)
library(devtools)
session_info()
convert_to_unit
convert_to_unit(123, unit="s")
microbenchmark:::convert_to_unit(time, unit="s")
microbenchmark
?microbenchmark
boxplot(time)
microbenchmark:::convert_to_unit(time, unit="s")
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L)
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[1]/10e6,
dtm.time = dtm$time[2]/10e6,
dtm.2g.time = dtm.2g$time[3]/10e6,
dtm.3g.time = dtm.3g$time[4]/10e6))
}
library(tm)
library(ggplot2)
library(dplyr)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L)
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[1]/10e6,
dtm.time = dtm$time[2]/10e6,
dtm.2g.time = dtm.2g$time[3]/10e6,
dtm.3g.time = dtm.3g$time[4]/10e6))
}
library(RWeka)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L)
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[1]/10e6,
dtm.time = dtm$time[2]/10e6,
dtm.2g.time = dtm.2g$time[3]/10e6,
dtm.3g.time = dtm.3g$time[4]/10e6))
}
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
library(reshape2)
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
time.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L)
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[1]/10e6,
dtm.time = dtm$time[2]/10e6,
dtm.2g.time = dtm.2g$time[3]/10e6,
dtm.3g.time = dtm.3g$time[4]/10e6))
}
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
time.record
time
time$time[1]/10e6
time$time[3]/10e6
time$time[2]/10e6
time$time
time$time/10e9
time$time
str(time)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L, unit = "s")
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[4],
dtm.time = dtm$time[3],
dtm.2g.time = dtm.2g$time[2],
dtm.3g.time = dtm.3g$time[1]))
}
time.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L, unit = "s")
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[4],
dtm.time = dtm$time[3],
dtm.2g.time = dtm.2g$time[2],
dtm.3g.time = dtm.3g$time[1]))
}
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
time.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L, unit = "s")
print(time$time)
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time$time[4],
dtm.time = dtm$time[3],
dtm.2g.time = dtm.2g$time[2],
dtm.3g.time = dtm.3g$time[1]))
}
time["time"]
time[[
"time"]]
time
as.data.frame(time)
as.matrix(time)
time$time
time
time[1,]
print.microbenchmark
?microbenchmark
print(time)
inspect(time)
str(time)
time$expr
time[time$expr, 2]
time.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L, unit = "s")
print(time$time)
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time[time$expr, "time"][1],
dtm.time = time[time$expr, "time"][2],
dtm.2g.time = time[time$expr, "time"][3],
dtm.3g.time = time[time$expr, "time"][4]))
}
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
time
time[time$expr, "time"][1]
time[time$expr, "time"][2]
time[time$expr, "time"][3]
time[time$expr, "time"][4]
time.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L, unit = "s")
print(time)
print(time[time$expr, "time"][1])
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time[time$expr, "time"][1],
dtm.time = time[time$expr, "time"][2],
dtm.2g.time = time[time$expr, "time"][3],
dtm.3g.time = time[time$expr, "time"][4]))
}
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
time.record <- data.frame(data.size=0, corpus.time=0, dtm.time=0, dtm.2g.time=0, dtm.3g.time=0)
for(i in seq(100, 1000, 100)){
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
TrigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 3, max = 3))
time <- microbenchmark(
news.corpus <- VCorpus(VectorSource(en_us.news[1:i])) %>% corpus.preprocess(),
news.dtm <- DocumentTermMatrix(news.corpus),
news.dtm.2g <- DocumentTermMatrix(news.corpus, control=list(tokenize = BigramTokenizer)),
news.dtm.3g <- DocumentTermMatrix(news.corpus, control=list(tokenize = TrigramTokenizer)),
times=1L, unit = "s")
browser()
time.record <- rbind(time.record,
data.frame(data.size = i,
corpus.time = time[time$expr, "time"][1],
dtm.time = time[time$expr, "time"][2],
dtm.2g.time = time[time$expr, "time"][3],
dtm.3g.time = time[time$expr, "time"][4]))
}
ggplot(melt(time.record, id="data.size", value.name="time.needed"), aes(x=data.size, y=time.needed, group=variable, color=variable)) +
geom_line(size=1) +
geom_point(size=4) +
theme_bw() +
ggtitle("Impact of data size on computation time")
time
q
q()
exit
exit(0)
exit()
quit()
q()
q
